{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2f6be4-4a66-44f4-82e1-fae60eacc680",
   "metadata": {},
   "source": [
    "# Two Interesting Concpets From Machine Learning\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creator:__ Fisayo Banjoko\n",
    "\n",
    "__Content reviewers:__ Padraig Gleeson, Ankur Singha\n",
    "\n",
    "__Content editors:__ Padraig Gleeson, Ankur Singha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd06455-7198-492c-8bc7-565cd44e1af4",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f368ef-452d-43fe-b96f-506698e13664",
   "metadata": {},
   "source": [
    "Computers have come a long way since the first electronic computers that were capable of performing mathematical calculations to the sophisticated devices we have today. Computational speed and memory of modern computers have dwarfed earlier computers and the capabilities of modern computers have now extended beyond mathematical calculations to cognitive tasks that until recently only humans were capable of performing. Now computers are capable of defeating professional chess players, detecting early signs of a disease, driving automobiles and so on. Computers are able to achieve these feats and others using **Machine Learning** which enables computer systems to learn and improve without being explicitly programmed to do so. The machine in machine learning here refers to devices with a Central Processing Unit (CPU) or Graphics Processing Unit (GPU) which is where machine learning models are built, tested and deployed. These devices can be a laptop or desktop, a remote server, a robot, an automobile and so on. \n",
    "\n",
    "Here are two interesting concepts from the fascinating field of Machine Learning. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d512306-9d92-4243-a6e3-ee5ef5328b91",
   "metadata": {},
   "source": [
    "<p align='center'><img src='https://i.insider.com/5a00b43e4d05ac0b3b8b682b?width=1000&format=jpeg&auto=webp?raw=True'/></p>\n",
    "\n",
    "_Figure 1. Graph showing the growth of machine learning in terms of memory and computational speed. This shows how close machines are to operating in similar manner as a rodent's brain and the computational power and memory needed. <br>\n",
    "Credits: i.insider.com_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5153bb-3958-4c79-b3c5-1383463f811b",
   "metadata": {},
   "source": [
    "### 1. Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a949733-87b2-4111-93dc-c9cecff54b24",
   "metadata": {},
   "source": [
    "Regression analysis is a simple supervised learning technique used to find the best trendline to\n",
    "describe a dataset. It is the primary technique to solving regression problems and can be said to be the equivalent to \"Hello World\" of Machine Learning algorithms. Regression analysis is primarily used to predict the value of the dependent variable (target) when information about the independent variables exist, or in order to identify the relation between the dependent variable and the independent variables in a dataset.\n",
    "\n",
    "There are different types of regression analysis techniques and the use of each technique depend on factors such as target variable, shape of the regression line, and the number of independent variables.<br>\n",
    "Regression analysis techniques include: <br>\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Lasso Regression\n",
    "4. Ridge Regression\n",
    "5. Polynimial Regression\n",
    "6. Bayesian Linear Regression <br>\n",
    "<br>\n",
    "Lets take some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c4684-3e75-416d-ae66-645683fa2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to install necessary libraries\n",
    "!pip install sklearn --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install statsmodels --quiet\n",
    "!pip install torchvision==0.9.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d7cae-f652-49c6-af98-14a7cd036bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85182e52-1dfd-42f8-91ec-872eca7e6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to load the dataset from a csv file using pandas\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Fifi-Banjoko/Outreachy/main/Bitcoin%20Price%20Example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a308e-c2d2-4c28-9e58-7cec5c1e7137",
   "metadata": {},
   "source": [
    "For example, the cell below shows a small dataset charting bitcoin values to the US dollar. It shows date, bitcoin value and the number of days that have transpired from the first recorded date. Using regression analysis we can predict the price of bitcoin on a later date that is not shown here. The dependent variable here is the bicoin value we are trying to predict and the independent variable here is the number of days transpired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c636e11-84c7-4b78-979c-569c24552c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to preview the entire datatset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88170481-ad0e-411a-99b3-b6f68f2a1561",
   "metadata": {},
   "source": [
    "The cell below shows this dataset represented on a scatterplot, with the dependent variable which is the _bitcoin price_ on the y axis and the independent variable which is _number of days trasnspired_ on the x axis. From the scatterplot, there is a steady increase in the bitcoin price as the number of days increases and a steep increase at the 736th day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c37b8-c75f-4698-9b12-1eb3ff6f1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['No of Days Transpired']\n",
    "y= df['Bitcoin Price']\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab427ec1-2fe0-4ad2-96cf-dd1731c30186",
   "metadata": {},
   "source": [
    "Using linear regression, you can easily predict future values of the bitcoin price. Linear regression comprises a straight line that splits your data points on a scatterplot. The goal of linear regression is to split your data in a way that minimizes the distance between the regression line and all data points on the scatterplot. <br>\n",
    "\n",
    "Plotting the linear regression line that is, *hyperplane* or *trendline* we can predict the bitcoin price at the 800th day as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5e295-2a1c-43e4-a25a-03f74e22f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to view the linear regression model built for the dataset on a scatter plot\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#simple linear regression model\n",
    "x= np.array (x).reshape((-1, 1))\n",
    "y= np.array (y)\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "model = LinearRegression().fit(x, y)\n",
    "r_sq = model.score(x, y)\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "#visualise the data \n",
    "plt.scatter(x, y, color = \"blue\")\n",
    "plt.plot(x, y_pred, color = \"green\")\n",
    "plt.title(\"Linear Regression Model\")\n",
    "plt.xlabel(\"No of Days Transpired\")\n",
    "plt.ylabel(\"Bitcoin Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36f96f-eba2-4bf5-b6bc-5c77900bf138",
   "metadata": {},
   "source": [
    "We can observe that according to the hyperplane, the bicoin price is predicted to drop on days past the 736th day, like the 800th day for example. This is due to the slope of the hyperplane and the point that previously made that linear regression seeks to minimise the the distance between the hyperplane and data points. This also shows that single or few outlying data points such as the bitcoin price on the 736th days also affect the prediction of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d03364-6f04-464a-81bb-2731b6ed65e6",
   "metadata": {},
   "source": [
    "This is a very simple linear regression model we have build to understand how regression analysis works. Datasets will typically be much larger than this with several hundred to hundreds of thousands of rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8cc18-64bf-420d-b859-b0a33f9fa6f3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d198a7e-69b0-48c3-8660-9990600d4749",
   "metadata": {},
   "source": [
    "### Coding Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479ad39-0db3-49ca-97dc-4a968a3a7b03",
   "metadata": {},
   "source": [
    "Now you can use regression analysis for determining the weather given a simple weather dataset where percentage chance of rain is charted to atmospheric temperature. Weather forecasts have more independent variables such as precipitation, atmospheric pressure and so on but only atmospheric temperature is used for simplicity sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b7d78-c34d-40ea-9550-39a898d06a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to load and preview the dataset\n",
    "df2 = pd.read_csv('https://raw.githubusercontent.com/Fifi-Banjoko/Outreachy/main/Book2.csv')\n",
    "df\n",
    "#dataFrame = pd.DataFrame(data=df, columns=['Bitcoin_Price','No_of_Days_Transpired'])\n",
    "x2= ___ #<---- Edit code\n",
    "y2= ___ #<---- Edit code\n",
    "plt.scatter(_________) #<---- Edit code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35ada8-d726-4a4c-b2c0-33e0530c464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the missing code below\n",
    "x2= np.array #<---- Edit code\n",
    "y2= np.array #<---- Edit code\n",
    "model = LinearRegression()\n",
    "model.fit(x2, y2)\n",
    "model = LinearRegression().fit(x2, y2)\n",
    "r_sq = model.score(x2, y2)\n",
    "y_pred2 = #<----- Edit code\n",
    "\n",
    "#Visualise your data\n",
    "plt.scatter(x2, y2, color = \"blue\")\n",
    "plt.plot(x2, y_pred2, color = \"green\")\n",
    "plt.title(\"Linear Regression Model\")\n",
    "plt.xlabel(\"________\") #<---- Edit code\n",
    "plt.ylabel(\"________\") #<---- Edit code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baaf202-4e88-478f-be89-60afdabf3191",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb8f5d-7ddb-4318-858c-fd6c7c17883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "df2 = pd.read_csv('https://raw.githubusercontent.com/Fifi-Banjoko/Outreachy/main/Book2.csv')\n",
    "#df2\n",
    "x2= df2['Atmospheric Temperature'] \n",
    "y2= df2['Percentage Chance of Rain']\n",
    "plt.scatter(x2,y2)\n",
    "plt.show()\n",
    "\n",
    "x2= np.array (x2).reshape((-1, 1))\n",
    "y2= np.array (y2)\n",
    "model = LinearRegression()\n",
    "model.fit(x2, y2)\n",
    "model2 = LinearRegression().fit(x2, y2)\n",
    "r_sq2 = model2.score(x2, y2)\n",
    "y_pred2 = model2.predict(x2)\n",
    "\n",
    "plt.scatter(x2, y2, color = \"blue\")\n",
    "plt.plot(x2, y_pred2, color = \"green\")\n",
    "plt.title(\"Linear Regression Model\")\n",
    "plt.xlabel(\"Atmospheric_Temperature\")\n",
    "plt.ylabel(\"Percentage_Chance_of_Rain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96837d-dfd0-4f18-af67-3fe7d168c4f2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5a994-20db-4524-a3ca-2e34cbecf2ce",
   "metadata": {},
   "source": [
    "Another common regression analysis technique is Logistic Regression. Logistic regression adopts the _sigmoid function_ to analyze data and predict discrete classes that exist in a dataset. Although logistic regression shares a visual resemblance to linear regression, it is technically a classification technique. Logistic regression is typically used for binary classification to predict two\n",
    "discrete classes, e.g. asleep or not asleep. The _sigmoid function_ produces an S-shaped curve that can convert any number and map it into a numerical value between 0 and 1, but it does so without ever reaching those exact limits. \n",
    "\n",
    "The sigmoid function is given by y = 1/1+e^-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199e1e7-29cc-4414-a6ea-f04f3eae24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the following data\n",
    "from scipy.optimize import curve_fit\n",
    "%matplotlib inline\n",
    "\n",
    "x3=np.arange(0,10.0)\n",
    "y3=np.array([52,133,203,230,237,239.5,239.8,239.9,240,240])\n",
    "\n",
    "def logifunc(x3,L,c,k):\n",
    "    return L/ (1 + c*np.exp(-k*x3))\n",
    "\n",
    "popt, pcov = curve_fit(logifunc, x3, y3, p0=[200,1,1])\n",
    "x_data = np.linspace(-10, 10, num=100)\n",
    "\n",
    "plt.scatter(x3,y3,label='Logistic function')\n",
    "plt.plot(x_data, logifunc(x_data, *popt), 'r-',label='Fitted function')\n",
    "plt.title(\"Logistic\")\n",
    "plt.xlabel('x3')\n",
    "plt.ylabel('y3')\n",
    "plt.xlim(-10,10)\n",
    "plt.ylim(0,250)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe0e8c-59ce-49c3-ad9b-b25d6148fa68",
   "metadata": {},
   "source": [
    "From the scatter plot above we can see that datapoints that have a low chance of occuring are closer to 0 which is impossibility and data points that have a higher cahnce of occuring are closet to 1 which is certain possibility. Depending on what the model is classifying, data points with a value above 0.5 are classified into a class and data points with a value below 0.5 are classified into another class. For example in classifying pregnant and non-pregnant women, women with data points that fall above 0.5 would be classified as pregnant and women with data points that fall below 0.5 would be classified as not pregnant. A data point with a value of 0.5 would be unclassifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb552a8f-67ad-4b22-9f2b-26b0333fd839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using sklearn\n",
    "\n",
    "#Importing the necessary libraries \n",
    "from sklearn.datasets import load_wine \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "\n",
    "# Importing the dataset from the sklearn library into a local variable called dataset\n",
    "dataset = load_wine()\n",
    "\n",
    "# Splitting the data test into train 80% and test 20%.\n",
    "# x_train, y_train are training data and labels respectively \n",
    "# x_test, y_test are testing data and labels respectively\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.20, random_state=15)\n",
    "\n",
    "# Making the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Training the model on the training data and labels\n",
    "logistic_model.fit(x_train, y_train)\n",
    "\n",
    "# Using the model to predict the labels of the test data\n",
    "y_predn = logistic_model.predict(x_test)\n",
    "\n",
    "# Evaluating the accuracy of the model using the sklearn accuracy function\n",
    "accuracy = accuracy_score(y_test,y_predn)*100\n",
    "\n",
    "# Evalutaing the number of true and false negatives and positives using the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test,y_predn)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Accuracy is\",accuracy)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d74b0-605f-48b5-8000-f9acab9977cb",
   "metadata": {},
   "source": [
    "Logistic Regression has various applications for classfication such as fraud detection, disease diagnosis, emergency detection, loan, default detection and so on. Although it is typically used for classification, it can also be used for prediction as we have seen in the model we just built. Also see cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2264334-1bd9-42b1-bd87-6b41131abcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to view the logistic regression plot used for prediction for a dataset\n",
    "df4 = pd.read_csv('https://raw.githubusercontent.com/Statology/Python-Guides/main/default.csv')\n",
    "df4[0:6] #preview the first 6 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b6744-6e9a-4e73-98b2-bf8b33747a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to view the scatter plot of the logistic regression\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "x4 = df4['balance']\n",
    "y4 = df4['default']\n",
    "sns.regplot(x=x4, y=y4, data=df4, logistic=True, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af28dfe-8a41-4c63-bf0e-3b9d94fa1d53",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f43f6-5e7b-42d4-916c-4304044d84c2",
   "metadata": {},
   "source": [
    "### Coding Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f5ca5-4f6b-47b2-882e-1bcd631b556b",
   "metadata": {},
   "source": [
    "Lets try a simple logistic regression model for predict digit labels based on images. After training the model with logistic regression, it can be used to predict an image label (labels 0–9) given an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553ee37-4537-4043-8b7a-e5e8ff3f61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries \n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Importing the dataset from the sklearn library into a local variable called digits\n",
    "digits = load_digits()\n",
    "\n",
    "# Print to show there are 1797 images (8 by 8 images for a dimensionality of 64)\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Print to show there are 1797 labels (integers from 0–9)\n",
    "print(\"Label Data Shape\", digits.target.shape)\n",
    "\n",
    "#visualise what the images and labels look like\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n",
    " plt.subplot(1, 5, index + 1)\n",
    " plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    " plt.title('Training: %i\\n' % label, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca865775-d7e8-4076-9ff5-09921e1e1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit code here \n",
    "\n",
    "# Splitting the data test into train 75% and test 25%.\n",
    "# x_train, y_train are training data and labels respectively \n",
    "# x_test, y_test are testing data and labels respectively\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)\n",
    "\n",
    "# Making the logistic regression model\n",
    "logisticRegr = ____________ #<---- Edit code here\n",
    "\n",
    "# Training the model on the training data and labels\n",
    "logisticRegr.fit(x_train2, y_train2)\n",
    "\n",
    "# Using the model to predict the labels of the test data\n",
    "logisticRegr.predict(x_test2[0].reshape(1,-1))\n",
    "predictions = _____________ #<---- Edit code here\n",
    "\n",
    "# Evaluating the accuracy of the model using the sklearn functions\n",
    "accuracy = ___________ #<---- Edit code here\n",
    "\n",
    "print(\"Accuracy is\",accuracy)\n",
    "#print(\"Confusion Matrix\")\n",
    "#print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7529f-b57a-4d89-ac37-8e99a903e52d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ef9f7-27ef-42af-973a-0e38b66cafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "\n",
    "# Splitting the data test into train 75% and test 25%.\n",
    "# x_train, y_train are training data and labels respectively \n",
    "# x_test, y_test are testing data and labels respectively\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)\n",
    "\n",
    "# Making the logistic regression model\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "# Training the model on the training data and labels\n",
    "logisticRegr.fit(x_train2, y_train2)\n",
    "\n",
    "# Using the model to predict the labels of the test data\n",
    "logisticRegr.predict(x_test2[0].reshape(1,-1))\n",
    "predictions = logisticRegr.predict(x_test2)\n",
    "\n",
    "# Evaluating the accuracy of the model using the sklearn functions\n",
    "accuracy = logisticRegr.score(x_test2, y_test2)\n",
    "\n",
    "print(\"Accuracy is\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3363a7-5cbe-476a-b475-427db2720e3b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb2cdb-c6d3-454a-aa97-d28327da411b",
   "metadata": {},
   "source": [
    "## 2. Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b2ac7-94ee-487e-a4fe-03f8eb0d255e",
   "metadata": {},
   "source": [
    "Artificial neural networks, also known as neural networks, is a popular machine learning technique to process data through layers of analysis. The naming of neural networks is because the algorithm learns by using interconnected nodes or neurons in a layered structure that resembles a human brain. Artificial neural networks are formed by interconnected neurons, also called _nodes_, which interact with each other\n",
    "through axons, called _edges_. This process is similar to how a neuron in the human brain functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8845b-8684-4e54-8a8e-5ce7df1762b5",
   "metadata": {},
   "source": [
    "Double click to see the brain's neural network and artificial neural network\n",
    "<p align='center'><img src='https://miro.medium.com/max/610/1*SJPacPhP4KDEB1AdhOFy_Q.png'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa5a26-efbb-409e-bc65-480dbf333e5b",
   "metadata": {},
   "source": [
    "In a neural network, the first layer typically consists of raw data such as numeric values, text, images or sound, which are divided into nodes. Each node then sends information to the next layer of nodes through the network’s edges. A typical neural network can be divided into _input_, _hidden_, and _output_ layers. Data is first received by the input layer, where broad features are detected, then the hidden layer(s) analyzes and processes the data. Based on previous computations, the data becomes streamlined through the passing of each hidden layer. The final result is shown as the output layer. The simplest method of assembling nodes in a neural network is a _feed forward_ network. That is, signals flow only in one direction and there is no loop in the network. The most basic form of a feed-forward neural network is the _perceptron_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033446ec-5cbe-4d1f-a5f9-f78fe29672d2",
   "metadata": {},
   "source": [
    "<p align='center'><img src='https://www.researchgate.net/profile/Zhenzhu-Meng/publication/339446790/figure/fig2/AS:862019817320450@1582532948784/A-biological-neuron-in-comparison-to-an-artificial-neural-network-a-human-neuron-b.png'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f67bdd-d1aa-4a5c-8744-b9cc6df5a917",
   "metadata": {},
   "source": [
    "Each edge has a numeric weight (algorithm) that can be altered and formulated based on experience. If the sum of the connected edges satisfies a set threshold, known as the activation function, it will activate a neuron at the next layer. However, if the sum of the connected edges does not meet the set threshold, the activation will not be triggered.<br>\n",
    "<br>\n",
    "Lets build a simple artificial neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d5beb-f16d-49ee-bc36-a8c87f23a792",
   "metadata": {},
   "source": [
    "First we would create a simple training and test set. Input and output training and test sets are created using NumPy’s array function, and input_pred is created to test a prediction function that will be defined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ed450-6b65-46dc-a44e-b9288b7db0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create a simple training and test dataset\n",
    "input_train = np.array([[0, 1, 0], [0, 1, 1], [0, 0, 0], \n",
    "                        [10, 0, 0], [10, 1, 1], [10, 0, 1]])\n",
    "output_train = np.array([[0], [0], [0], [1], [1], [1]])\n",
    "input_pred = np.array([1, 1, 0])\n",
    "\n",
    "input_test = np.array([[1, 1, 1], [10, 0, 1], [0, 1, 10], \n",
    "                       [10, 1, 10], [0, 0, 0], [0, 1, 1]])\n",
    "output_test = np.array([[0], [1], [0], [1], [0], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e727b3-c6aa-41e6-b977-3221d8880bc9",
   "metadata": {},
   "source": [
    "This next step is feature scaling, this step is optional for this example but advised when building more complex neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc74d6-4948-4bf4-aa72-d38d7854794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "input_train_scaled = scaler.fit_transform(input_train)\n",
    "output_train_scaled = scaler.fit_transform(output_train)\n",
    "input_test_scaled = scaler.fit_transform(input_test)\n",
    "output_test_scaled = scaler.fit_transform(output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cea5f-840b-4b00-8885-50f5391361f3",
   "metadata": {},
   "source": [
    "Next we create a class and call on the __init__ function. This function initializes variables that describes the size of the network and its weights. Input and output size are the numbers of input and output nodes and are equal to the number of features in our input and output data. Similarly, hidden size is the number of nodes in the hidden layer. Error list contains the mean absolute error (MAE) for each of the epochs. Limit describes the vector boundary and the true/ false positives and negatives variables store the number of true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71febfa-1345-4b64-acea-1c5468820eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to create a neural network class \n",
    "class NeuralNetwork():\n",
    "    def __init__(self, ):\n",
    "        self.inputSize = 3\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "\n",
    "        self.W1 = np.random.rand(self.inputSize, self.hiddenSize)\n",
    "        self.W2 = np.random.rand(self.hiddenSize, self.outputSize)\n",
    "\n",
    "        self.error_list = []\n",
    "        self.limit = 0.5\n",
    "        self.true_positives = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_negatives = 0\n",
    "        self.false_negatives = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d45c76-06aa-45e9-9029-19098f84f333",
   "metadata": {},
   "source": [
    "Next we create a forward pass propagaton function. This function iterates forward through different layers of the neural network to predict the output of an epoch. The Sigmiod activation function was chosen for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570ced2-abbe-49a6-8727-221a98cee93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run thus cell to create a forward propagation function\n",
    "def forward(self, X):\n",
    "        self.z = np.matmul(X, self.W1)\n",
    "        self.z2 = self.sigmoid(self.z)\n",
    "        self.z3 = np.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3)\n",
    "        return o\n",
    "\n",
    "def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "\n",
    "def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf6d32-68b4-47b4-bf4e-412dc6653d85",
   "metadata": {},
   "source": [
    "For the network to train, the model’s predicted output is compared to the actual output and the difference between these two results is measured and is known as the _cost_ or _cost value_. Ideally we want this cost value to be as low as possible, which would mean that we have a good model. Reducing the cost value is achieved by incrementally tweaking the network’s weights until the lowest possible cost\n",
    "value is obtained. This process of training the neural network is called _backpropagation_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a11a2e-db3e-4965-9dbd-231b56956a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run thus cell to create a backward propagation function\n",
    "def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        self.z2_error = np.matmul(self.o_delta, np.matrix.transpose(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += np.matmul(np.matrix.transpose(X), self.z2_delta)\n",
    "        self.W2 += np.matmul(np.matrix.transpose(self.z2), self.o_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b401d57-617e-430b-a2ed-03cda5be2fa0",
   "metadata": {},
   "source": [
    "Next we create a _training function_ so the algorithm can run the forward and backward propagation for the number of epochs we have chosen. We also create a _prediction function_ so the algorithm can predict the output for new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bb5f0-8714-408c-9b21-393c19b54a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to create a training function and a prediction function \n",
    "def train(self, X, y, epochs):\n",
    "    for epoch in range(epochs):\n",
    "            o = self.forward(X)\n",
    "            self.backward(X, y, o)\n",
    "            self.error_list.append(np.abs(self.o_error).mean())\n",
    "def predict(self, x_predicted):\n",
    "    return self.forward(x_predicted).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3ec30-cf7f-4fb8-b837-598a1123d4ae",
   "metadata": {},
   "source": [
    "Next we plot the mean absolute error that we have previously used in our code to monitor error development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736d32d-bf12-4a0f-b823-94b97bbe984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_error_development(self):\n",
    "        plt.plot(range(len(self.error_list)), self.error_list)\n",
    "        plt.title('Mean Sum Squared Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831344f-c38b-4af3-9f62-edb67b4e772d",
   "metadata": {},
   "source": [
    "We need to calculate the accuracy and the components of the accuracy such as number of true positive and true negatives to get an idea of how well our model is doing next. A high number of false positives or negatives indicate a poor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72cb7d-a278-4a8e-a26c-09b8a1d95279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation(self, input_test, output_test):\n",
    "        for i, test_element in enumerate(input_test):\n",
    "            if self.predict(test_element) > self.limit and \\\n",
    "                    output_test[i] == 1:\n",
    "                self.true_positives += 1\n",
    "            if self.predict(test_element) < self.limit and \\\n",
    "                    output_test[i] == 1:\n",
    "                self.false_negatives += 1\n",
    "            if self.predict(test_element) > self.limit and \\\n",
    "                    output_test[i] == 0:\n",
    "                self.false_positives += 1\n",
    "            if self.predict(test_element) < self.limit and \\\n",
    "                    output_test[i] == 0:\n",
    "                self.true_negatives += 1\n",
    "        print('True positives: ', self.true_positives,\n",
    "              '\\nTrue negatives: ', self.true_negatives,\n",
    "              '\\nFalse positives: ', self.false_positives,\n",
    "              '\\nFalse negatives: ', self.false_negatives,\n",
    "              '\\nAccuracy: ',\n",
    "              (self.true_positives + self.true_negatives) /\n",
    "              (self.true_positives + self.true_negatives +\n",
    "               self.false_positives + self.false_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8074d4-08bf-4046-be1f-23596d3f98a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, we create a script to test the neural network we just built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186e82a-7435-482e-bb4b-6850008aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork()\n",
    "NN.train(input_train_scaled, output_train_scaled, 200)\n",
    "NN.predict(input_pred)\n",
    "NN.view_error_development()\n",
    "NN.test_evaluation(input_test_scaled, output_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2815fbd-6cf9-4e47-8a99-bfe0eb46d50d",
   "metadata": {},
   "source": [
    "Note that this is a small dataset and small datasets are prone to _overfitting_. Overfitting occurs when a function is too closely aligned to a limited set of data points. As a result, the model is useful in reference only to its initial data set and has a high accuracy for the dataset but not to any other data sets. Overfitting is a result of high variance (how scattered your predicted values are) and low bias (the gap between your predicted value and the actual value). Since the output accurcy is 1.0, the model ia likely overfit to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631fd3f-6d34-45eb-89b2-cb43f7a343f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full code\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, ):\n",
    "        self.inputSize = 3\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "\n",
    "        self.W1 = np.random.rand(self.inputSize, self.hiddenSize)\n",
    "        self.W2 = np.random.rand(self.hiddenSize, self.outputSize)\n",
    "\n",
    "        self.error_list = []\n",
    "        self.limit = 0.5\n",
    "        self.true_positives = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_negatives = 0\n",
    "        self.false_negatives = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = np.matmul(X, self.W1)\n",
    "        self.z2 = self.sigmoid(self.z)\n",
    "        self.z3 = np.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3)\n",
    "        return o\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        self.z2_error = np.matmul(self.o_delta,\n",
    "                                  np.matrix.transpose(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += np.matmul(np.matrix.transpose(X), self.z2_delta)\n",
    "        self.W2 += np.matmul(np.matrix.transpose(self.z2),\n",
    "                             self.o_delta)\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            o = self.forward(X)\n",
    "            self.backward(X, y, o)\n",
    "            self.error_list.append(np.abs(self.o_error).mean())\n",
    "\n",
    "    def predict(self, x_predicted):\n",
    "        return self.forward(x_predicted).item()\n",
    "\n",
    "    def view_error_development(self):\n",
    "        plt.plot(range(len(self.error_list)), self.error_list)\n",
    "        plt.title('Mean Sum Squared Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "    def test_evaluation(self, input_test, output_test):\n",
    "        for i, test_element in enumerate(input_test):\n",
    "            if self.predict(test_element) > self.limit and \\\n",
    "                    output_test[i] == 1:\n",
    "                self.true_positives += 1\n",
    "            if self.predict(test_element) < self.limit and \\\n",
    "                    output_test[i] == 1:\n",
    "                self.false_negatives += 1\n",
    "            if self.predict(test_element) > self.limit and \\\n",
    "                    output_test[i] == 0:\n",
    "                self.false_positives += 1\n",
    "            if self.predict(test_element) < self.limit and \\\n",
    "                    output_test[i] == 0:\n",
    "                self.true_negatives += 1\n",
    "        print('True positives: ', self.true_positives,\n",
    "              '\\nTrue negatives: ', self.true_negatives,\n",
    "              '\\nFalse positives: ', self.false_positives,\n",
    "              '\\nFalse negatives: ', self.false_negatives,\n",
    "              '\\nAccuracy: ',\n",
    "              (self.true_positives + self.true_negatives) /\n",
    "              (self.true_positives + self.true_negatives +\n",
    "               self.false_positives + self.false_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cdc12-d0ab-4d2d-a506-28253f0b635f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f19de-035b-49f9-9157-50450e31a884",
   "metadata": {},
   "source": [
    "Using a much larger dataset and the scikit-learn module lets take another example. This example is a classification model to detect diabetes using a Multilayer Perceptron (MLP), which is a fully connected class of feedforward artificial neural network (ANN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35881a24-d7a9-4dd0-b405-2d19df0e8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neccessary libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95216745-6c4a-479f-b888-22ca5e1fbf70",
   "metadata": {},
   "source": [
    "After importing the necessary libraries and modules, the data is loaded from a csv file called `diabetes.csv` using the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce3354-621b-4109-9335-adb00a60c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Fifi-Banjoko/Outreachy/main/diabetes.csv') \n",
    "print(df.shape)\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b18eaa-dd90-4e8e-84ad-7fcb07dfd1c0",
   "metadata": {},
   "source": [
    "Next, create an object an object of the target variable called the *target column*. Then create a variable *predictors* which contains a list of all the features excluding the the target variable. Then scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11485c1e-e1c7-47a8-8e0a-64d12d8e6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of the target variable called target_column.\n",
    "target_column = ['Outcome'] \n",
    "#create a variable 'predictors' that contains a list of all the features \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "#scale the features between 0 and 1\n",
    "df[predictors] = df[predictors]/df[predictors].max()\n",
    "#view the features\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23681b-0ded-49cf-9c2e-6087414cba0b",
   "metadata": {},
   "source": [
    "Next create arrays for the independent variable X and the dependent variable Y. Then split the dataset into training and test data sets. The test set is set to be 25% so the training set is 75%. We can also print the shapes of both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9aba46-f6a8-48db-b9e5-029aac488a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pandas to create arrays for the independent variable X and the dependent variable y \n",
    "X = df[predictors].values\n",
    "Y = (df[target_column].values.reshape(-1,))\n",
    "\n",
    "#split the dataset into traininf and test data sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe39551-103c-487b-9eb8-fd681cd1f721",
   "metadata": {},
   "source": [
    "Using the multilayer perceptron classifier from sklearn we build the neural network choosing the number of hiddden layers, activation function, solver and maximum iterations. The hidden layer here is set to 3 layers, each with 8 neurons. The *rectified linear acivation function (ReLU)* is chosen as the activation function. This activation function is simple, fast and works well with neural networks. The *Adam Solver* is an optimization algorithm similar to stochastic gradient descent for training deep learning models. Maximum iterations is set to 500. Then, we fit the model to the training data and make predictions on the training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f9d57-ef2c-464c-b2ab-e1c7b54478e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of hidden layers, activation function, optimizer and max number of iterations\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500)\n",
    "#fit the model to the training data\n",
    "mlp.fit(X_train,Y_train)\n",
    "#make predictions on the training and test sets\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898c621-49a1-489e-b3d8-3ee6538c6df4",
   "metadata": {},
   "source": [
    "Lastly, after the predictions are made, the performance of our model is evaluated using sklearn metrics. You can print the accuracy of the model or print the *confusion or error matrix* which is the summary of the prediction results of a classification problem and helps visualoze the performace of an algorithm. From the confusion matrix gotten, we can see the model's true positive, false positive, false negative and true negative predictions. Hence, we can see how well the model has classified if a person has diabetes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa9c85-abcf-4f1e-9015-b56b5a159d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "#print the accuracy of the model using sklearn accuracy_score function\n",
    "print (\"Accuracy of training data is\", accuracy_score(Y_train, predict_train))\n",
    "print (\"Accuracy of test data is\", accuracy_score(Y_test, predict_test))\n",
    "\n",
    "#print the confusion matrix that has the number of true and false positives and negatives on the training data\n",
    "print(confusion_matrix(Y_train,predict_train))\n",
    "#print the confusion matrix that has the number of true and false positives and negatives on the test data\n",
    "print(confusion_matrix(Y_test,predict_test))\n",
    "\n",
    "#Uncomment to get the classification report on the training and test data\n",
    "#print(classification_report(Y_train,predict_train))\n",
    "#print(classification_report(Y_test,predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21d4b8-38e8-4753-b335-c8812db13801",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e93edc-672e-46c9-8641-626ceae51229",
   "metadata": {},
   "source": [
    "## Coding Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e473e2f-5c02-48e1-b4ce-eed72b369d7e",
   "metadata": {},
   "source": [
    "For this coding exercise, you will tweak some parts of the code to get better accuracy and confusion matrix. You may change the number of neurons, hidden layers, activation function, solver, training and test split and maximum iterations. <br> \n",
    "Here is a guide to choosing activation functions for neural networks: https://www.v7labs.com/blog/neural-networks-activation-functions <br>\n",
    "Here is a guide to choosing solvers for neural networks: https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17b1bf-3390-4ff3-812c-45702639812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full code\n",
    "\n",
    "#create an object of the target variable called target_column.\n",
    "target_column = ['Outcome'] \n",
    "#create a variable 'predictors' that contains a list of all the features \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "#scale the features between 0 and 1\n",
    "df[predictors] = df[predictors]/df[predictors].max()\n",
    "#view the features\n",
    "df.describe().transpose()\n",
    "#use pandas to create arrays for the independent variable X and the dependent variable y \n",
    "X = df[predictors].values\n",
    "Y = (df[target_column].values.reshape(-1,))\n",
    "\n",
    "#splirt the dataset into traininf and test data sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=40) # <---- Edit code here\n",
    "print(X_train.shape); print(X_test.shape)\n",
    "\n",
    "#set the number of hidden layers, activation function, optimizer and max number of iterations\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='identity', solver='lbfgs', max_iter=500) # <---- Edit code here\n",
    "#fit the model to the training data\n",
    "mlp.fit(X_train,Y_train)\n",
    "#make predictions on the training and test sets\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)\n",
    "\n",
    "##print the accuracy of the model using sklearn accuracy_score function\n",
    "print (\"Accuracy of training data is\", accuracy_score(Y_train, predict_train))\n",
    "print (\"Accuracy of test data is\", accuracy_score(Y_test, predict_test))\n",
    "\n",
    "#print the confusion matrix that has the number of true and false positives and negatives on the training data\n",
    "print(confusion_matrix(Y_train,predict_train))\n",
    "#print the confusion matrix that has the number of true and false positives and negatives on the test data\n",
    "print(confusion_matrix(Y_test,predict_test))\n",
    "\n",
    "#Uncomment to get the classification report on the training and test data\n",
    "#print(classification_report(Y_train,predict_train))\n",
    "#print(classification_report(Y_test,predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b32ed-398f-4861-88f7-79d8e18f6a7e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c8dd8-90a2-498f-ac23-48bdd0a77578",
   "metadata": {},
   "source": [
    "Thank you for reading this notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
